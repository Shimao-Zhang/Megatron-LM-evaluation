protocolVersion: 2
name: evaluate_test_
type: job
jobRetryCount: 0
prerequisites:
  - type: dockerimage
    uri: >-
      bench.azurecr.io/superbench:sigma-megatron-with-msccl-large-scale-opt-nccl-2.23
    name: docker_image0
taskRoles:
  worker:
    instances: 1
    completion:
      minFailedInstances: 1
    taskRetryCount: 0
    dockerImage: docker_image0
    extraContainerOptions:
      shmMB: 16384
      infiniband: true
    resourcePerInstance:
      gpu: 8
      cpu: 88
      memoryMB: 819200
    commands:
      - echo "Prepare NCCL env"
      - mkdir -p /opt/microsoft
      - >-
        wget
        https://raw.githubusercontent.com/Azure/azhpc-images/master/topology/ndv4-topo.xml
        -O /opt/microsoft/ndv4-topo.xml
      - export NCCL_IB_PCI_RELAXED_ORDERING=1
      - export NCCL_SOCKET_IFNAME=eth0
      - export CUDA_DEVICE_ORDER=PCI_BUS_ID
      - export NCCL_NET_GDR_LEVEL=5
      - export NCCL_TOPO_FILE=/opt/microsoft/ndv4-topo.xml
      - export NCCL_DEBUG=WARN
      - nvidia-smi
      - echo "Install blobfuse..."
      - >-
        wget
        https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.deb
      - dpkg -i packages-microsoft-prod.deb
      - apt -y update
      - apt --fix-broken install -y
      - apt -y upgrade
      - apt-get -y update
      - apt-get -y upgrade --fix-missing
      - apt-get -y install libfuse3-dev fuse3 --fix-missing
      - apt-get -y install blobfuse2 --fix-missing
      - echo "Download code"
      - cd /mnt
      - pwd
      - >-
        git clone --branch main
        https://github.com/Shimao-Zhang/Megatron-LM-evaluation.git
      - cd Megatron-LM-evaluation
      - ls -alh .
      - echo "Prepare other envs"
      - pip install transformers
      - pip install deepspeed
      - pip install h5py
      - pip install wandb
      - pip install hf_xet
      - pip install lm_eval
      - echo "[Used Envs]"
      - pip list
      - export AZUREML_NODE_COUNT=$PAI_TASK_ROLE_TASK_COUNT_worker
      - export GPUS_PER_NODE=8
      - export MASTER_ADDR=$PAI_HOST_IP_worker_0
      - export MASTER_PORT=$PAI_PORT_LIST_worker_0_http
      - export NUM_NODES=$PAI_TASK_ROLE_TASK_COUNT_worker
      - export NODE_RANK=$PAI_CURRENT_TASK_ROLE_CURRENT_TASK_INDEX
      - 'export LD_LIBRARY_PATH=/opt/hpcx/ucx/lib:$LD_LIBRARY_PATH'
      - echo "Add shimao-zhang-blob"
      - python /mnt/Megatron-LM-evaluation/manual_mount_zhangsm.py
      - echo "Copy data"
      - mkdir /scratch/
      - mkdir /scratch/torch_model
      - chmod 777 /scratch/torch_model
      # - mkdir /scratch/proof_pile_2-open_web_math
      # - chmod 777 /scratch/proof_pile_2-open_web_math
      # - mkdir /scratch/redpajama-arxiv
      # - chmod 777 /scratch/redpajama-arxiv
      # - mkdir /scratch/wiki
      # - chmod 777 /scratch/wiki
      # - mkdir /scratch/dclm_data
      # - chmod 777 /scratch/dclm_data
      # - mkdir /scratch/starcoder
      # - chmod 777 /scratch/starcoder
      # - mkdir /scratch/pes2o
      # - chmod 777 /scratch/pes2o
      
      - bash evaluate.sh
defaults:
  virtualCluster: default
extras:
  com.microsoft.pai.runtimeplugin:
    - plugin: ssh
      parameters:
        jobssh: true
        userssh:
          type: custom
          value: ''
    - plugin: teamwise_storage
      parameters:
        storageConfigNames:
          - blob-openpai-xiaoliuinterns
          - blob-openpai-xiaoliuinterns-out
          - blob-hptrainingwesteurope-pretraining
          - blob-pretraining-hptrainingwestcentralus
  hivedScheduler:
    taskRoles:
      worker:
        skuNum: 8
        skuType: A100
  jobStatusChangeNotification:
    running: true
    succeeded: true
    failed: true
