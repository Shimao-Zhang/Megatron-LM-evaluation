protocolVersion: 2
name: mtp_torch_to_hf
type: job
jobRetryCount: 0
prerequisites:
  - type: dockerimage
    uri: >-
      bench.azurecr.io/superbench:sigma-megatron-with-msccl-large-scale-opt-nccl-2.23
    name: docker_image0
taskRoles:
  worker:
    instances: 1
    completion:
      minFailedInstances: 1
    taskRetryCount: 0
    dockerImage: docker_image0
    extraContainerOptions:
      shmMB: 16384
      infiniband: true
    resourcePerInstance:
      gpu: 8
      cpu: 88
      memoryMB: 819200
    commands:
      - echo "Prepare NCCL env"
      - mkdir -p /opt/microsoft
      - >-
        wget
        https://raw.githubusercontent.com/Azure/azhpc-images/master/topology/ndv4-topo.xml
        -O /opt/microsoft/ndv4-topo.xml
      - export NCCL_IB_PCI_RELAXED_ORDERING=1
      - export NCCL_SOCKET_IFNAME=eth0
      - export CUDA_DEVICE_ORDER=PCI_BUS_ID
      - export NCCL_NET_GDR_LEVEL=5
      - export NCCL_TOPO_FILE=/opt/microsoft/ndv4-topo.xml
      - export NCCL_DEBUG=WARN
      - nvidia-smi
      - echo "Download code"
      - git clone https://github.com/SteveKGYang/Megatron-LM.git
      - cd Megatron-LM
      - ls -alh .
      - echo "Prepare other envs"
      - pip install transformers
      - pip install h5py
      - pip install wandb
      - echo "[Used Envs]"
      - pip list
      - export AZUREML_NODE_COUNT=$PAI_TASK_ROLE_TASK_COUNT_worker
      - export GPUS_PER_NODE=8
      - export MASTER_ADDR=$PAI_HOST_IP_worker_0
      - export MASTER_PORT=$PAI_PORT_LIST_worker_0_http
      - export NUM_NODES=$PAI_TASK_ROLE_TASK_COUNT_worker
      - export NODE_RANK=$PAI_CURRENT_TASK_ROLE_CURRENT_TASK_INDEX
      - 'export LD_LIBRARY_PATH=/opt/hpcx/ucx/lib:$LD_LIBRARY_PATH'
      # - echo "Copy data"
      - mkdir /scratch/
      - mkdir /scratch/torch_model
      - chmod 777 /scratch/torch_model
      # - mkdir /scratch/proof_pile_2-open_web_math
      # - chmod 777 /scratch/proof_pile_2-open_web_math
      # - mkdir /scratch/redpajama-arxiv
      # - chmod 777 /scratch/redpajama-arxiv
      # - mkdir /scratch/wiki
      # - chmod 777 /scratch/wiki
      # - mkdir /scratch/dclm_data
      # - chmod 777 /scratch/dclm_data
      # - mkdir /scratch/starcoder
      # - chmod 777 /scratch/starcoder
      # - mkdir /scratch/pes2o
      # - chmod 777 /scratch/pes2o
      # - bash copy_dataset_olmo2.sh
      - LOAD_DIR="/mnt/pvc-blob-nfs/klyang/tuning_result/llama_3B_data_evaluation_dclm_0215/"
      - TARGET_DIR="/mnt/pvc-blob-nfs/klyang/hf_converted_model/llama_3B_data_evaluation_dclm_0215/checkpoint-48000/"
      - echo "Start converting..."
      - python tools/checkpoint/convert.py --loader core --model-type GPT --position-embedding-type rope --load-dir $LOAD_DIR --save-dir /scratch/torch_model --target-tensor-parallel-size 1 --target-pipeline-parallel-size 1 --megatron-path ./
      - python weights_conversion/megatron_to_hf.py --input_dir /scratch/torch_model --num_output_shards 1 --output_dir $TARGET_DIR
      - PATTERN="tokenizer*.json"
      - cp /mnt/pvc-blob-nfs/xiaoliu2/Sigma1-10b/GK4V16-Q6144-C4096-M10B-lr5e-5-B16M-Phiv2-1016-retry4-90k/$PATTERN $TARGET_DIR
      - echo "End converting..."
defaults:
  virtualCluster: default
extras:
  com.microsoft.pai.runtimeplugin:
    - plugin: ssh
      parameters:
        jobssh: true
        userssh:
          type: custom
          value: ''
    - plugin: teamwise_storage
      parameters:
        storageConfigNames:
          - pvc-blob-nfs
  hivedScheduler:
    jobPriorityClass: prod
    taskRoles:
      worker:
        skuNum: 8
        skuType: A100
  jobStatusChangeNotification:
    running: true
    succeeded: true
    failed: true
