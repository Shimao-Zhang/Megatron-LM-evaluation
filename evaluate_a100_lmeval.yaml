protocolVersion: 2
name: evaluate_test_
type: job
jobRetryCount: 0
prerequisites:
  - type: dockerimage
    uri: >-
      bench.azurecr.io/superbench:sigma-megatron-with-msccl-large-scale-opt-nccl-2.23
    name: docker_image0
parameters:
  checkpoint_name: OpenPAI-Pretrain-17BA3B-RoPE-HQ-0405
  iteration: 196000
  output_path: /mnt/shimao-zhang-blob/multi-mix/evaluation_results/OpenPAI-Pretrain-17BA3B-RoPE-HQ-0405/hf_iter_196000
taskRoles:
  worker:
    instances: 1
    completion:
      minFailedInstances: 1
    taskRetryCount: 0
    dockerImage: docker_image0
    extraContainerOptions:
      shmMB: 16384
      infiniband: true
    resourcePerInstance:
      gpu: 8
      cpu: 88
      memoryMB: 819200
    commands:
      - echo "Prepare NCCL env"
      - mkdir -p /opt/microsoft
      - >-
        wget
        https://raw.githubusercontent.com/Azure/azhpc-images/master/topology/ndv4-topo.xml
        -O /opt/microsoft/ndv4-topo.xml
      - export NCCL_IB_PCI_RELAXED_ORDERING=1
      - export NCCL_SOCKET_IFNAME=eth0
      - export CUDA_DEVICE_ORDER=PCI_BUS_ID
      - export NCCL_NET_GDR_LEVEL=5
      - export NCCL_TOPO_FILE=/opt/microsoft/ndv4-topo.xml
      - export NCCL_DEBUG=WARN
      - nvidia-smi
      - echo "Install blobfuse..."
      - >-
        wget
        https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.deb
      - dpkg -i packages-microsoft-prod.deb
      - apt -y update
      - apt --fix-broken install -y
      - apt -y upgrade
      - apt-get -y update
      - apt-get -y upgrade --fix-missing
      - apt-get -y install libfuse3-dev fuse3 --fix-missing
      - apt-get -y install blobfuse2 --fix-missing
      - echo "Download code"
      - cd /mnt
      - pwd
      - >-
        git clone --branch main
        https://github.com/Shimao-Zhang/Megatron-LM-evaluation.git
      - cd Megatron-LM-evaluation
      - ls -alh .
      - echo "Prepare other envs"
      - pip install transformers
      - pip install deepspeed
      - pip install h5py
      - pip install wandb
      - pip install hf_xet
      - pip install lm_eval
      - echo "[Used Envs]"
      - pip list
      - export AZUREML_NODE_COUNT=$PAI_TASK_ROLE_TASK_COUNT_worker
      - export GPUS_PER_NODE=8
      - export MASTER_ADDR=$PAI_HOST_IP_worker_0
      - export MASTER_PORT=$PAI_PORT_LIST_worker_0_http
      - export NUM_NODES=$PAI_TASK_ROLE_TASK_COUNT_worker
      - export NODE_RANK=$PAI_CURRENT_TASK_ROLE_CURRENT_TASK_INDEX
      - 'export LD_LIBRARY_PATH=/opt/hpcx/ucx/lib:$LD_LIBRARY_PATH'
      - echo "Add shimao-zhang-blob"
      - python /mnt/Megatron-LM-evaluation/manual_mount_zhangsm.py
      - echo "Copy data"
      - mkdir /scratch/
      - mkdir /scratch/torch_model
      - chmod 777 /scratch/torch_model
      
      - export CHECKPOINT_NAME=<% $parameters.checkpoint_name %>
      - export ITERATION_STR=<% $parameters.iteration %>
      - export HUGGINGFACE_CKPT_PATH="/mnt/blob-pretraining-hptrainingwestcentralus-out/checkpoints/${CHECKPOINT_NAME}/hf_iter_${ITERATION_STR}"
      - export HUGGINGFACE_CKPT_DIRNAME=$(basename $HUGGINGFACE_CKPT_PATH)
      - export OUTPUT_PATH=<% $parameters.output_path %>
      - echo "evaluation..."
      - mkdir -p "$OUTPUT_PATH"
      - >-
        lm_eval
        --model hf
        --model_args pretrained="$HUGGINGFACE_CKPT_PATH",dtype="bfloat16"
        --batch_size 2
        --num-fewshot 5
        --trust_remote_code
        --tasks ceval-valid
        --output_path "$OUTPUT_PATH/eval"
        2>&1 | tee "$OUTPUT_PATH/eval.txt"
      - echo "evaluation done"
      - date
defaults:
  virtualCluster: default
extras:
  com.microsoft.pai.runtimeplugin:
    - plugin: ssh
      parameters:
        jobssh: true
        userssh:
          type: custom
          value: ''
    - plugin: teamwise_storage
      parameters:
        storageConfigNames:
          - blob-openpai-xiaoliuinterns
          - blob-openpai-xiaoliuinterns-out
          - blob-hptrainingwesteurope-pretraining
          - blob-pretraining-hptrainingwestcentralus
  hivedScheduler:
    taskRoles:
      worker:
        skuNum: 8
        skuType: A100
  jobStatusChangeNotification:
    running: false
    succeeded: false
    failed: true
